<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>The blog of Eric Chan</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000"/>
 <updated>2019-05-19T07:41:02-05:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Eric Chan</name>
   <email>eric.chan.24@gmail.com</email>
 </author>

 
 <entry>
   <title>What Are People Saying About Airlines on Twitter?</title>
   <link href="http://localhost:4000/topic%20modeling/2019/05/18/What-Are-People-Saying-About-Airlines-on-Twitter/"/>
   <updated>2019-05-18T00:00:00-05:00</updated>
   <id>http://localhost:4000/topic%20modeling/2019/05/18/What-Are-People-Saying-About-Airlines-on-Twitter</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-18-What-Are-People-Saying-About-Airlines-on-Twitter/thumbnails.png&quot; alt=&quot;Thumbnails&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction-and-goal&quot;&gt;Introduction and Goal&lt;/h2&gt;
&lt;p&gt;Given tweets that mention an airline company, can we understand what people are saying about that company and their experiences with them? I pulled two weeks worth of tweets mentioning airlines for analysis using machine learning Natural Language Processing (NLP) techniques.&lt;/p&gt;

&lt;p&gt;The final goal is a list of topics that people are tweeting about. This is an unsupervised learning problem because we don’t have labels for the topics we are finding, nor do we even know the number of topics to look for. These are things that will be determined during the modeling process.&lt;/p&gt;

&lt;h2 id=&quot;modeling-process-and-pipeline&quot;&gt;Modeling Process and Pipeline&lt;/h2&gt;
&lt;p&gt;Let me guide you through my process in going from tens of thousands of tweets to a list of topics. First I cleaned my tweets. Twitter data is messy and filled with non-standard English words. There are slang, misspellings, abbreviations, hashtags, Twitter handles, links, emojis, and other things that make it difficult to analyze. A thorough cleaning is imperative.&lt;/p&gt;

&lt;p&gt;I made my own custom cleaning function to handle Twitter specific issues like correcting common misspellings, slang, eliminating emojis, Twitter handles etc. . .&lt;/p&gt;

&lt;p style=&quot;float: left; font-size: 9pt; text-align: center; width: 99%;
margin-right: 1%; margin-bottom: 0.5em;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-05-May/2019-05-18-What-Are-People-Saying-About-Airlines-on-Twitter/raw_tweet.png&quot; style=&quot;width: 100%&quot; /&gt;
Raw tweet, words with red squares will be cleaned&lt;/p&gt;

&lt;p style=&quot;float: left; font-size: 9pt; text-align: center; width: 99%;
margin-right: 1%; margin-bottom: 0.5em;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-05-May/2019-05-18-What-Are-People-Saying-About-Airlines-on-Twitter/cleaned_tweet.png&quot; style=&quot;width: 100%&quot; /&gt;
After the first iteration of cleaning the tweet&lt;/p&gt;

&lt;p&gt;This is the first of many iterations in the cleaning process. From here I would convert all words to lower case, remove “$” symbols, remove numbers, and other things to make tweets easier for a computer to work with.&lt;/p&gt;

&lt;p&gt;Next I took the cleaned tweets and converted them to a numerical format.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-18-What-Are-People-Saying-About-Airlines-on-Twitter/count_vectorizer.png&quot; alt=&quot;Count Vectorizer&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each row is a tweet and the words in the tweet would be tallied in the corresponding columns.&lt;/p&gt;

&lt;p&gt;After counting, I had a really, really long list of words and counts. I had a matrix that was&lt;/p&gt;

&lt;p&gt;61711 rows x 24583 columns&lt;/p&gt;

&lt;p&gt;The rows represent the number of tweets, the columns represent the number of unique words. How do we work with this data?&lt;/p&gt;

&lt;p&gt;We need to do dimensionality reduction. I like to think of dimensionality reduction as taking a lot of data that doesn’t mean much on its own and compressing it down such that the compressed data conveys much more information. Take movies for example. There are tens of thousands of movies, each one having its own unique features. We can go from a space where we think about every movie individually to one where we think in genres. Suddenly, we go from tens of thousands of movies to 30 genres.&lt;/p&gt;

&lt;p&gt;Similarly with words, we can take tens of thousands of words and compress them into “topics.”&lt;/p&gt;

&lt;p&gt;It turns out, it is much easier to analyze tweets in this reduced dimension “topic” space. In the new “topic” space, I can group our tweets into clusters.&lt;/p&gt;

&lt;p&gt;After clustering, I examine the tweets in each cluster and try to understand the “topic” of the cluster and if it makes sense.&lt;/p&gt;

&lt;p&gt;This process is repeated until the topics make sense. It is an iterative process. There is no right answer or score.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-18-What-Are-People-Saying-About-Airlines-on-Twitter/workflow.png&quot; alt=&quot;Workflow&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above is the flow chart for working the project. Start at the upper left. The process is iterative, which means it is repeated until the data scientist is satisfied with the results.&lt;/p&gt;

&lt;h2 id=&quot;summary-and-app&quot;&gt;Summary and App&lt;/h2&gt;
&lt;p&gt;One particularly interesting topic that I found was topic number 9 which was about racism. On October 25, American Airlines was flagged for racist policies. There were a huge spike in tweets directed towards them from angry tweeters. However, the controversy quickly died down and the angry tweets returned to normal within a few days.&lt;/p&gt;

&lt;p&gt;Another notable topic was topic number 3 which was about love of flying. It showed people’s enthusiasm for traveling and their positive experiences with airlines. Topic number 7 is a topic nearly every traveler is familiar with: waiting at the gate, worrying about time, and flight delays.&lt;/p&gt;

&lt;p&gt;Here is a link to an &lt;a href=&quot;https://airline-tweets.herokuapp.com/&quot; target=&quot;_blank&quot;&gt;Interactive App&lt;/a&gt; that summarizes my results.&lt;/p&gt;

&lt;p&gt;The code for the project can be found on my &lt;a href=&quot;https://github.com/ericchan24/airline_twitter&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Predicting Fake News With 99 Percent Accuracy</title>
   <link href="http://localhost:4000/classification/naive%20bayes/2019/05/17/Predicting-Fake-News-With-99-percent-Accuracy/"/>
   <updated>2019-05-17T00:00:00-05:00</updated>
   <id>http://localhost:4000/classification/naive%20bayes/2019/05/17/Predicting-Fake-News-With-99-percent-Accuracy</id>
   <content type="html">&lt;h2 id=&quot;tldr&quot;&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;I built a classifier using AI and natural language processing that was 99% accurate in detecting whether news articles were real or fake news. I obtained the articles by web scraping real and fake news sites to download over 600 articles.&lt;/p&gt;

&lt;p style=&quot;float: left; font-size: 9pt; text-align: center; width: 49%;
margin-right: 1%; margin-bottom: 0.5em;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-05-May/2019-05-17-Predicting-Fake-News-With-99-Percent-Accuracy/real_news_wordcloud.png&quot; style=&quot;width: 100%&quot; /&gt;
Real News Words&lt;/p&gt;

&lt;p style=&quot;float: left; font-size: 9pt; text-align: center; width: 49%;
margin-right: 1%; margin-bottom: 0.5em;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-05-May/2019-05-17-Predicting-Fake-News-With-99-Percent-Accuracy/fake_news_wordcloud.png&quot; style=&quot;width: 100%&quot; /&gt;
Fake News Words&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A large chemical company keeps its employees up to date on news by compiling a list of news articles to send to its staff. Recently, its news feed has been infiltrated by fake news articles. I used AI and Natural Language Processing to build a classifier to separate the genuine articles from the fake news articles.&lt;/p&gt;

&lt;h2 id=&quot;the-blacklist&quot;&gt;The Blacklist&lt;/h2&gt;
&lt;p&gt;The company provided a list of sites on its “blacklist”. These were sites that they flagged as sources of fake news. At first glance, articles on these sites appear to be real news. But closer examination reveals otherwise. Here is an excerpt of an article from one of the sites on the blacklist &lt;a href=&quot;http://nbpostgazette.com/&quot; target=&quot;_blank&quot;&gt;nbpostgazette&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote cite=&quot;http://nbpostgazette.com&quot;&gt;
Growing opportunity for startups in the Fuel Dispensers industry&lt;br /&gt;&lt;br /&gt;

Questale released a detailed assessment of trends in Fuel Dispensers market. The research report includes diverse topics like total market size, key market drivers, challenges, growth opportunities, key players etc. We have also covered key market updates, the impact of regulations and technological updates. New startups entering the space of Fuel Dispensers need to carefully pick their niches and genres so that they can compete on an equal footing with global companies who have an end to end development studios, production capabilities and global skills and experience backing them.

You can get free access to samples from the report here: https://questale.com/report/fuel-dispensers-market-report-by-company-regions-types-and-applications-global-status-and-forecast-to-2025/303479

&lt;/blockquote&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;building-a-data-set&quot;&gt;Building a data set&lt;/h2&gt;
&lt;p&gt;I built a web scraper to download over 600 news articles from real news sources (e.g. NY Times, FiveThirtyEight, Reuters) and fake news sources using the company’s blacklist (e.g. NBPostGazette, JournalismDay, SatPRNews).&lt;/p&gt;

&lt;p&gt;I prepared the articles for building a classifier by removing stop words (words such as &lt;strong&gt;and&lt;/strong&gt;, &lt;strong&gt;the&lt;/strong&gt;, and &lt;strong&gt;I&lt;/strong&gt; that don’t convey much information), converting all words to lowercase, and removing punctuation.&lt;/p&gt;

&lt;h2 id=&quot;counting-the-words-in-each-article&quot;&gt;Counting the words in each article&lt;/h2&gt;
&lt;p&gt;Counting the words in each article is the key step that quantifies our text data into a numerical format for modeling.&lt;/p&gt;

&lt;p&gt;I used a method of quantifying article word counts called term frequency inverse document frequency (TF-IDF).&lt;/p&gt;

&lt;p&gt;Term frequency represents how important each word is in an article. It is a rate statistic.&lt;/p&gt;

&lt;p&gt;Inverse document frequency penalizes words that appear in many articles. The idea is that words that occur in many articles are less informative than those
that occur only in a small number of the articles.&lt;/p&gt;

&lt;p&gt;TF-IDF multiplies term frequency by inverse document frequency to quantify the importance of each word in each article.&lt;/p&gt;

&lt;h2 id=&quot;naive-bayes&quot;&gt;Naive Bayes&lt;/h2&gt;
&lt;p&gt;Naive Bayes is a fast, accurate, and easy to implement classifier often used in text classification. It is based on Bayes’ theorem and the &lt;strong&gt;naive&lt;/strong&gt; part comes from the assumption that the features in the dataset are independent.&lt;/p&gt;

&lt;p&gt;We are interested in calculating the posterior probabilities: What is the
probability that an article is fake news given the words in the article?&lt;/p&gt;

&lt;p&gt;Naive Bayes looks at the TF-IDF score for each article and uses those numbers to identify words correlated with real news and fake news.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;The classifier misclassified one out of 127 articles in the test set. It correctly classified all 27 fake news articles in the test set.&lt;/p&gt;

&lt;p&gt;This is a &lt;a href=&quot;https://github.com/ericchan24/Fake-News-Classifier&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt; link with all the code used in this project.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Zion National Park Should be a UNESCO World Heritage Site</title>
   <link href="http://localhost:4000/classification/support%20vector%20machines/2019/05/16/Zion-National-Park-Should-Be-a-UNESCO-World-Heritage-Site/"/>
   <updated>2019-05-16T00:00:00-05:00</updated>
   <id>http://localhost:4000/classification/support%20vector%20machines/2019/05/16/Zion-National-Park-Should-Be-a-UNESCO-World-Heritage-Site</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;\assets\img\2019-05-May\2019-05-16-Zion-National-Park-Should-be-a-UNESCO-World-Heritage-Site\montage.png&quot; alt=&quot;Montage&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Zion National Park is a large and beautiful park in Utah. Millions of visitors visit annually to hike its scenic terrain. Why is Zion not on the list of UNESCO World Heritage Sites?&lt;/p&gt;

&lt;h2 id=&quot;what-is-unesco&quot;&gt;What is UNESCO?&lt;/h2&gt;

&lt;p&gt;UNESCO is an agency of the United Nations that promotes science, education, and culture. One of the ways they do this is with a list of World Heritage Sites. To make it on the list, a site must be judged of significance to the collective interests of humanity.&lt;/p&gt;

&lt;p&gt;I built a model using machine learning to classify whether a site would make it on the UNESCO list.&lt;/p&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;
&lt;p&gt;UNESCO sites are divided into two types, one is cultural. These are man-made sites such as The Statue of Liberty or The Great Wall of China. The other type is natural. Natural sites are parks, marine areas, and landforms that exhibit outstanding examples plant and animal life. I focused on the natural sites, specifically the parks. The parks have more consistent features that can be used to build a model.&lt;/p&gt;

&lt;h2 id=&quot;getting-the-data&quot;&gt;Getting the Data&lt;/h2&gt;
&lt;p&gt;I gathered data from UNESCO, the US National Park Service, the Canadian National Park Service, and TripAdvisor for reviews about each park. The US and Canadian Park Service gave me observations of parks not on the UNESCO list.&lt;/p&gt;

&lt;h2 id=&quot;features&quot;&gt;Features&lt;/h2&gt;
&lt;p&gt;The features that I looked at were size of the park, age of the park, TripAdvisor review score and Number of TripAdvisor reviews. The number of reviews was important because some sites had a very high rating.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-16-Zion-National-Park-Should-be-a-UNESCO-World-Heritage-Site/torngat_trip_advisor.PNG&quot; alt=&quot;High Rating&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But only one review.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-16-Zion-National-Park-Should-be-a-UNESCO-World-Heritage-Site/torngat_trip_advisor1.PNG&quot; alt=&quot;One Review&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So I combined review score and number of reviews into a single metric “weighted reviews” that accounts for the number of reviews. This also serves as proxy as number of visitors to a park that is important in the model.&lt;/p&gt;

&lt;h2 id=&quot;testing-out-machine-learning-algorithms&quot;&gt;Testing out Machine Learning Algorithms&lt;/h2&gt;
&lt;p&gt;I used F1 score as my scoring metric. A baseline model that always predicts a park is a UNESCO site had an F1 score of 49%. I trained six different machine learning models with my data and here are the results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-16-Zion-National-Park-Should-be-a-UNESCO-World-Heritage-Site/f1_score_by_model.PNG&quot; alt=&quot;F1 Scores by Model&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A support vector machine (SVM) model was the best performer with a F1 score of 62%.&lt;/p&gt;

&lt;h2 id=&quot;what-exactly-can-this-model-do&quot;&gt;What Exactly Can This Model Do?&lt;/h2&gt;
&lt;p&gt;Given a set of park features, I can input them into the model will look at the data I used to train it and output a prediction whether a park is a UNESCO World Heritage Site.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-16-Zion-National-Park-Should-be-a-UNESCO-World-Heritage-Site/prediction.png&quot; alt=&quot;Prediction&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The model predicts that Zion National Park should be classified as a UNESCO World Heritage Site.&lt;/p&gt;

&lt;h2 id=&quot;going-forward&quot;&gt;Going Forward&lt;/h2&gt;
&lt;p&gt;The model currently has a F1 score of 62% on 140 parks, 70 that were UNESCO sites and 70 that were non-UNESCO sites. I would like to add more parks from all around the world to see if I can increase the F1 score.&lt;/p&gt;

&lt;p&gt;With a strong enough score, the model would be used to find those parks that are not currently UNESCO sites that should be UNESCO sites.&lt;/p&gt;

&lt;p&gt;The code for the project can be found on my &lt;a href=&quot;https://github.com/ericchan24/unesco_classification&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Classifying Animals Using Machine Learning</title>
   <link href="http://localhost:4000/classification/random%20forest/2019/05/15/Classifying-Animals-Using-Machine-Learning/"/>
   <updated>2019-05-15T00:00:00-05:00</updated>
   <id>http://localhost:4000/classification/random%20forest/2019/05/15/Classifying-Animals-Using-Machine-Learning</id>
   <content type="html">&lt;head&gt;
&lt;style&gt;
.wrap {
    width: 300px;
    position: relative;
}

.wrap img {
    float: left;
    height: 20px;
}

.wrap h2 {
    line-height: 20px;
    &lt;!-- top: 33px;
    left: 50px; --&gt;
    &lt;!-- display: inline; --&gt;
}
tr.dark {
    background-color: #141866;
    color: #ffffff;
}
&lt;/style&gt;
&lt;/head&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\img\2019-05-May\2019-05-15-Classifying-Animals-Using-Machine-Learning\mini_highland_cow.png&quot; alt=&quot;Mini Highland Cow&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is a mini highland cow. From the image, we see that it has hair, a tail,
and four legs. Some other things that we know about this animal is that it has
teeth and it produces milk.&lt;/p&gt;

&lt;p&gt;The mini highland cow’s attributes are what are called the features of the
animal. The class it belongs to is the mammal class. My goal is to develop a
model that maps from the features of an animal to its class.&lt;/p&gt;

&lt;h2 id=&quot;the-data&quot;&gt;The Data&lt;/h2&gt;

&lt;p&gt;The Data
I used a data set from the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/zoo&quot; target=&quot;_blank&quot;&gt;UCI Machine Learning repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\assets\img\2019-05-May\2019-05-15-Classifying-Animals-Using-Machine-Learning\animal_classes.png&quot; alt=&quot;Animal Classes&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The animals are divided into seven classes. A particularly interesting class is
the non-insect invertebrate class. It includes are a variety of animals from
worms, arachnids, crustaceans, mollusks, and others.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\assets\img\2019-05-May\2019-05-15-Classifying-Animals-Using-Machine-Learning\number_of_animals_by_class.PNG&quot; alt=&quot;Number of Animals by Class&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are 101 animals in the data set. There were only five reptiles and four
amphibians. This is important to keep in mind during the splitting of the data.
I want at least one example of reptile and amphibian in my testing data.&lt;/p&gt;

&lt;h2 id=&quot;data-exploration-preprocessing-and-cleaning&quot;&gt;Data exploration, preprocessing, and cleaning&lt;/h2&gt;
&lt;p&gt;This is a clean data set with no missing values or outliers. The only non-binary feature was the number of legs of each animal. I scaled the number of legs to values between 0 and 1 using a max-min scaler.&lt;/p&gt;

&lt;h2 id=&quot;modeling&quot;&gt;Modeling&lt;/h2&gt;
&lt;p&gt;There are many algorithms to choose for classification. There is a rule of thumb for logistic regression requiring ten training observations per feature to have properly calibrated model probabilities. I decided to not use logistic regression. Naive Bayes has the assumption that the features used are independent. Since I had features such as “Does this animal have fins” and “Does this animal live in the water”, my features were not independent. I decided not to use naive bayes.&lt;/p&gt;

&lt;p&gt;I built four models and compared the results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\assets\img\2019-05-May\2019-05-15-Classifying-Animals-Using-Machine-Learning\four_model_accuracy_barh.PNG&quot; alt=&quot;Four Model Accuracy&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These models were built without any tuning of hyperparameters. Random forest
and decision tree were producing high accuracy and moved forward with these
algorithms.&lt;/p&gt;

&lt;p&gt;I used randomized grid search to tune the hyperparameters for a decision tree
and random forest model. Random forest produced the best results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\assets\img\2019-05-May\2019-05-15-Classifying-Animals-Using-Machine-Learning\confusion_matrix_random_forest.PNG&quot; alt=&quot;Confusion Matrix Random Forest&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The random forest model correctly classified every observation in the test set.&lt;/p&gt;

&lt;p&gt;Conclusion
I made a model that maps from animal features to the class of that animal using
random forest. The model was trained on 81 observations and tested on 20 observations. It had 100% accuracy on the test set.&lt;/p&gt;

&lt;p&gt;I made an &lt;a href=&quot;https://animal-classifier.herokuapp.com/&quot; target=&quot;_blank&quot;&gt;interactive app&lt;/a&gt; that shows how the model works.&lt;/p&gt;

&lt;p&gt;The code for this project can be found on my &lt;a href=&quot;https://github.com/ericchan24/Animal-Classification&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How Expensive is Your City?</title>
   <link href="http://localhost:4000/linear%20regression/2019/05/14/How-Expensive-is-Your-City/"/>
   <updated>2019-05-14T00:00:00-05:00</updated>
   <id>http://localhost:4000/linear%20regression/2019/05/14/How-Expensive-is-Your-City</id>
   <content type="html">&lt;head&gt;
&lt;style&gt;
.wrap {
    width: 300px;
    position: relative;
}

.wrap img {
    float: left;
    height: 20px;
}

.wrap h2 {
    line-height: 20px;
    &lt;!-- top: 33px;
    left: 50px; --&gt;
    &lt;!-- display: inline; --&gt;
}
tr.dark {
    background-color: #141866;
    color: #ffffff;
}
&lt;/style&gt;
&lt;/head&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Suppose you receive a job offer in a new city. Your new job will give you a raise by 10%. But will you be better off financially after moving to that city after taking into consideration its cost of living?&lt;/p&gt;

&lt;p&gt;The website &lt;a href=&quot;https://www.expatistan.com/cost-of-living&quot; target=&quot;_blank&quot;&gt;Expatistan&lt;/a&gt; has information about prices from over 320 cities around the world. Each city has a list of over 50 items and their corresponding prices. The website takes those items and prices and gives us a value for the price index of each city.&lt;/p&gt;

&lt;h2 id=&quot;objective&quot;&gt;Objective&lt;/h2&gt;
&lt;p&gt;I decided to take the idea of this website and extend it by building a model to predict the cost of living index of any city in the world.&lt;/p&gt;

&lt;p&gt;In order train my model, I needed to get data. I used Python’s BeautifulSoup library to build a web scraper, to scrape 320 individual city pages from Expatistan’s website. I extracted the item names and item prices from each page and loaded the data into a Pandas data frame.&lt;/p&gt;

&lt;p&gt;I cleaned up the data and built my model. I used scikit-learn to perform a linear regression to calculate the coefficients for my model. Here are the results of my model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-14-How-Expensive-is-Your-City/all_features.png&quot; alt=&quot;All Features Model Results&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I reverse engineered the website’s model. I was able to perfectly predict a city’s price index!&lt;/p&gt;

&lt;p&gt;However, my goal was to be able to use this model on all cities, not only the cities included on the website. The problem with this is that I need 50 prices of all sorts of items from in categories from food, housing, clothing, transportation, health, and entertainment. For some of these items would be nearly impossible to get their prices. There is no reliable to place to find the price of &lt;strong&gt;1 pair of men’s leather business shoes&lt;/strong&gt; or &lt;strong&gt;Short visit to private Doctor&lt;/strong&gt;. I went item-by-item and kept only the items for which I could easily find their prices.&lt;/p&gt;

&lt;p&gt;I was left with only six items: gasoline, rental housing, fast food, public transportation, internet, and automobiles. I had to build a new model with the combination of these items. I also created a new feature called “region” that was based on the location of the city because I thought location would affect a city’s price index.&lt;/p&gt;

&lt;p&gt;I was skeptical whether a new model using only seven items would be very predictive. I noticed that rent was highly correlated to price index and I built a test model using only this single feature. It turns out that rent by itself is highly predictive of price index! From here, I added the price of fast food, the price of a monthly bus pass, and the region in which the city was located as my final model.&lt;/p&gt;

&lt;p&gt;Here are the results of my final model:
&lt;img src=&quot;/assets/img/2019-05-May/2019-05-14-How-Expensive-is-Your-City/final_predictions.png&quot; alt=&quot;Final Model&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;chicago-vs-south-bend&quot;&gt;Chicago vs South Bend&lt;/h2&gt;
&lt;p&gt;Here’s an example of the model in action. I’ll calculate the price indices for Chicago and compare it to South Bend.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# chicago
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;12.76&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Rent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.03&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Bus_ticket&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;105&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.42&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Fast_Food&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.57&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Region&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;14.43&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cost_chi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Rent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bus_ticket&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Fast_Food&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Region&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cost_chi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;184&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# south bend
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Intercept_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;12.76&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Rent_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;900&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.03&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Bus_ticket_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;36&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.42&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Fast_Food_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.57&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Region_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;14.43&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cost_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Intercept_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Rent_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bus_ticket_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Fast_Food_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Region_sb&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cost_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;109&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cost_chi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.688&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;My model calculates that Chicago’s cost of living is about 69% more expensive than South Bend’s.&lt;/p&gt;

&lt;p&gt;The code for the project can be found on my &lt;a href=&quot;https://github.com/ericchan24/price_indices&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here’s a link to some &lt;a href=&quot;https://price-indices.herokuapp.com/&quot; target=&quot;_blank&quot;&gt;visualizations&lt;/a&gt; that I made for this project.&lt;/p&gt;
</content>
 </entry>
 
 
</feed>
