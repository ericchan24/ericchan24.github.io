<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>The blog of Eric Chan</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000"/>
 <updated>2020-05-24T10:05:22-05:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Eric Chan</name>
   <email>eric.chan.24@gmail.com</email>
 </author>

 
 <entry>
   <title>Automate your tasks with AWS Lambda</title>
   <link href="http://localhost:4000/automation/aws/lambda/web%20scraping/2020/05/20/Automate-Your-Tasks-With-AWS-Lambda/"/>
   <updated>2020-05-20T00:00:00-05:00</updated>
   <id>http://localhost:4000/automation/aws/lambda/web%20scraping/2020/05/20/Automate-Your-Tasks-With-AWS-Lambda</id>
   <content type="html">&lt;h2 id=&quot;introduction-and-goal&quot;&gt;Introduction and Goal&lt;/h2&gt;
&lt;p&gt;In a previous post, I &lt;a href=&quot;https://ericchan24.github.io/automation/cron/email/web%20scraping/2019/11/30/Keep-Up-With-the-News-Using-Python/&quot; target=&quot;_blank&quot;&gt;kept up with the news using Python&lt;/a&gt;.
I utilized Cron on my local machine to scheudle and run a Python script to
scrape and grab all Fangraphs articles for the day.&lt;/p&gt;

&lt;p&gt;This works well when my local machine is turned on and available to run the
script. If not, the script fails to run and I miss out on the latest
baseball analysis from Fangraphs.&lt;/p&gt;

&lt;p&gt;A better way to do this is with AWS Lambda. Lambda is a serverless cloud
computing service and is perfect for this task. To get this running, I’ll have
to write some code that will be run by Lambda and stored in an S3 bucket.&lt;/p&gt;

&lt;h2 id=&quot;tools-used&quot;&gt;Tools used&lt;/h2&gt;
&lt;p&gt;AWS lambda&lt;/p&gt;

&lt;h2 id=&quot;create-a-s3-bucket&quot;&gt;Create a S3 Bucket&lt;/h2&gt;
&lt;p&gt;Create a S3 bucket to store the articles. From the main AWS page, click on
the Services tab on the top of the screen. Type S3 in the search bar and click
on S3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/s3_photo.png&quot; alt=&quot;S3&quot; width=&quot;550px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Create a bucket. I named my bucket fangraphs.&lt;/p&gt;

&lt;h2 id=&quot;initialize-lambda-function&quot;&gt;Initialize Lambda Function&lt;/h2&gt;
&lt;p&gt;From AWS, click on the Services dropdown and type in Lambda.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/lambda_photo.png&quot; alt=&quot;Lambda&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Click Create Function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/create_function.png&quot; alt=&quot;Create Function&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Select Author from scratch and enter a Function name. Choose your programming
language and click Create Function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/create_function1.png&quot; alt=&quot;Create Function1&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;write-lambda-function-code&quot;&gt;Write Lambda Function code&lt;/h2&gt;
&lt;p&gt;Write your lambda function code. Name your script lambda_function.py containing
a function named lambda_handler().&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# lambda_function.py&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lambda_handler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;event&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# write code here&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The code that I wrote grabs all Fangraphs articles for the day, converts them
to a text object and saves it to my S3 bucket. This is a link to the
&lt;a href=&quot;https://github.com/ericchan24/automate_your_tasks_with_aws_lambda/blob/master/scripts/lambda_function.py&quot; target=&quot;_blank&quot;&gt;code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here’s one tricky part of the code. You will need your AWS credentials to access
your S3 bucket. The code below shows how I did that.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;region&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'AWS_REGION'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;access_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'AWS_ACCESS_KEY'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;secret_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'AWS_SECRET_KEY'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# access your S3 bucket&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boto3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resource&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'s3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;region_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;region&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;aws_access_key_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;access_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;aws_secret_access_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;secret_key&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# place the articles in the S3 bucket&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fangraphs'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Body&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These are the &lt;a href=&quot;https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html&quot; target=&quot;_blank&quot;&gt;environment variables&lt;/a&gt;
that Lambda can access.&lt;/p&gt;

&lt;h2 id=&quot;install-packages-and-create-a-zip-file&quot;&gt;Install Packages and Create a Zip File&lt;/h2&gt;
&lt;p&gt;If your code uses packages outside of the base Python installation, you will
have to create a zip file containing those packages and upload them to Lambda.
Here are the &lt;a href=&quot;https://gist.github.com/gene1wood/4a052f39490fae00e0c3&quot; target=&quot;_blank&quot;&gt;packages&lt;/a&gt;
available in AWS Lambda environments.&lt;/p&gt;

&lt;p&gt;In addition to the Python base installation, my code used BeautifulSoup, lxml,
and requests.&lt;/p&gt;

&lt;p&gt;Make sure these files are installed in the same directory as your code.
To do this, navigate to the directory where your code is and type the following:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install beautifulsoup4 &lt;span class=&quot;nt&quot;&gt;--target&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
pip install lxml &lt;span class=&quot;nt&quot;&gt;--target&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
pip install requests &lt;span class=&quot;nt&quot;&gt;--target&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When you have all your packages and code ready to go, zip all these files
together. Navigate to the directory where your lambda_function.py file is
located and type the following:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;zip &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; ../lambda_function.zip &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Saving the zip file one level up from the lambda_function.py file worked for me.&lt;/p&gt;

&lt;h2 id=&quot;upload-zip-file&quot;&gt;Upload Zip File&lt;/h2&gt;
&lt;p&gt;Back on the AWS Lambda Function screen, navigate to the Function code section.
Under the Code entry type dropdown, select Upload a .zip file and choose your
zip file.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/upload_zip.png&quot; alt=&quot;Upload Zip&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;set-permissions&quot;&gt;Set Permissions&lt;/h2&gt;
&lt;p&gt;Permission needs to be granted to your Lambda Function to allow access to your
S3 bucket. Click the Services dropdown and type IAM. Click on IAM Manage access
to AWS resources.&lt;/p&gt;

&lt;p&gt;On the IAM Dashboard, under Access Management, select Role. Click the Create
Role buttom.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/IAM.png&quot; alt=&quot;IAM&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On Create role page 1, select Lambda and click on the Next: Permissions
button on the bottom of the page.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/IAM1.png&quot; alt=&quot;IAM1&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On Create role page 2, type S3 in search box and select AmazonS3FullAccess.
Click the Next: Tags button on the bottom of the page.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/IAM2.png&quot; alt=&quot;IAM2&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On Create role page 3, click Next: Review&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/IAM3.png&quot; alt=&quot;IAM3&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On Create role page 4, Type in a Role name and description. Click the
Create role button on the bottom of the screen.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/IAM4.png&quot; alt=&quot;IAM4&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Go back to your Lambda Function and selec thte Permissions Tab. Under Execution
role, click the Edit button.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/IAM5.png&quot; alt=&quot;IAM5&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Increase your Timeout. Under Existing role, select the role that you just
created. Click the Save button.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/IAM6.png&quot; alt=&quot;IAM6&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Click the Permissions tab and verify that the Execution role has been set
correctly.
&lt;img src=&quot;/assets/img/2020-05-May/20/IAM7.png&quot; alt=&quot;IAM7&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;test-your-code&quot;&gt;Test Your Code&lt;/h2&gt;
&lt;p&gt;After setting up permisisons, test your code. Click the Test button in the top
right of the Lambda Function dashboard.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/code_test.png&quot; alt=&quot;Code Test&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AWS Lambda will tell you whether or not your test succeeded with execution logs
for tracing your runs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/code_test1.png&quot; alt=&quot;Code Test1&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;My code is writing a file to my S3 bucket, so I navigated to the bucket to
verify that a file was written there.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/s3_photo1.png&quot; alt=&quot;S3 Photo1&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;schedule-your-code-to-execute&quot;&gt;Schedule Your Code to Execute&lt;/h2&gt;
&lt;p&gt;On the Lambda Function dashboard, under CloudWatch Events/EventBridge click Add
Trigger.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/cloudwatch.png&quot; alt=&quot;CloudWatch&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the Add trigger menu, select CloudWatch Events/EventBridge and Create a
new rule. I named my rule FangraphsDaily, gave it a description and used a Cron
expression for my daily run. Click the Enable trigger checkbox.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2020-05-May/20/cloudwatch1.png&quot; alt=&quot;CloudWatch&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;auomation-complete&quot;&gt;Auomation Complete!&lt;/h2&gt;
&lt;p&gt;You have set up a Lambda Function to automatically run a script and save the
results to a S3 bucket. Instead of running and scheduling this job locally,
take advantage of AWS Lambda to automate your tasks!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Keep up with the news using Python</title>
   <link href="http://localhost:4000/automation/cron/email/web%20scraping/2019/11/30/Keep-Up-With-the-News-Using-Python/"/>
   <updated>2019-11-30T00:00:00-06:00</updated>
   <id>http://localhost:4000/automation/cron/email/web%20scraping/2019/11/30/Keep-Up-With-the-News-Using-Python</id>
   <content type="html">&lt;h2 id=&quot;introduction-and-goal&quot;&gt;Introduction and Goal&lt;/h2&gt;
&lt;p&gt;I try to keep up with all the articles on
&lt;a href=&quot;https://fangraphs.com&quot; target=&quot;_blank&quot;&gt;Fangraphs&lt;/a&gt; every day. But some days,
I don’t have time to read read everything.&lt;/p&gt;

&lt;p&gt;For these days, I created an automated script to scrape all the articles on
Fangraphs and send an email attachment containing the articles.&lt;/p&gt;

&lt;h2 id=&quot;tools-used&quot;&gt;Tools used&lt;/h2&gt;
&lt;p&gt;Cron and BeautifulSoup&lt;/p&gt;

&lt;h2 id=&quot;building-the-scraper&quot;&gt;Building the scraper&lt;/h2&gt;
&lt;p&gt;To scrape the articles, I used the package BeautifulSoup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-11-November/30/beautiful_soup.jpg&quot; alt=&quot;Thumbnails&quot; width=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the workflow:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Find all the recent Fangraphs article links&lt;/li&gt;
  &lt;li&gt;Parse the article dates to find today’s articles&lt;/li&gt;
  &lt;li&gt;Write the articles to a text file&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the challenges with parsing and writing the articles was handling
non-utf-8 symbols in the articles. To handle this, I used the string function&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'utf-8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;to get rid of all the extra symbols.&lt;/p&gt;

&lt;h2 id=&quot;sending-the-email&quot;&gt;Sending the email&lt;/h2&gt;
&lt;p&gt;To send the email, I used the email library.&lt;/p&gt;

&lt;p&gt;I had to enable &lt;a href=&quot;https://support.google.com/accounts/answer/6010255?hl=en&quot; target=&quot;_blank&quot;&gt;Less secure app access&lt;/a&gt;
on my gmail account for the script to work.&lt;/p&gt;

&lt;h2 id=&quot;automating-the-script&quot;&gt;Automating the script&lt;/h2&gt;
&lt;p&gt;I used the software utility &lt;a href=&quot;https://en.wikipedia.org/wiki/Cron&quot; target=&quot;_blank&quot;&gt;cron&lt;/a&gt;
to run the script daily.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# scrape fangraphs articles daily at 11 pm&lt;/span&gt;
0 23 &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bash_profile&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; /Users/eric/anaconda/envs/python3.6/bin/python /Users/eric/ds/fangraphs/scripts/c_run_all.py &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; /Users/eric/ds/fangraphs/scripts/fangraphs.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I since I used environment variables and a conda virtual enviornment in the
script, I had to prepend the following the following commands to my Python
script call:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# activate enviornment variables&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.bash_profile
&lt;span class=&quot;c&quot;&gt;# use a conda virtual environment to run the script&lt;/span&gt;
/Users/eric/anaconda/envs/python3.6/bin/python
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;I created an automated process to run daily that scrapes all Fangraphs articles,
write them to a file, and send an email attachement with the articles.&lt;/p&gt;

&lt;p&gt;The code for the project can be found on my &lt;a href=&quot;https://github.com/ericchan24/keep_up_with_the_news&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>What Are People Saying About Airlines on Twitter?</title>
   <link href="http://localhost:4000/topic%20modeling/2019/05/18/What-Are-People-Saying-About-Airlines-on-Twitter/"/>
   <updated>2019-05-18T00:00:00-05:00</updated>
   <id>http://localhost:4000/topic%20modeling/2019/05/18/What-Are-People-Saying-About-Airlines-on-Twitter</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-18-What-Are-People-Saying-About-Airlines-on-Twitter/thumbnails.png&quot; alt=&quot;Thumbnails&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction-and-goal&quot;&gt;Introduction and Goal&lt;/h2&gt;
&lt;p&gt;Given tweets that mention an airline company, can we understand what people are saying about that company and their experiences with them? I pulled two weeks worth of tweets mentioning airlines for analysis using machine learning Natural Language Processing (NLP) techniques.&lt;/p&gt;

&lt;p&gt;The final goal is a list of topics that people are tweeting about. This is an unsupervised learning problem because we don’t have labels for the topics we are finding, nor do we even know the number of topics to look for. These are things that will be determined during the modeling process.&lt;/p&gt;

&lt;h2 id=&quot;modeling-process-and-pipeline&quot;&gt;Modeling Process and Pipeline&lt;/h2&gt;
&lt;p&gt;Let me guide you through my process in going from tens of thousands of tweets to a list of topics. First I cleaned my tweets. Twitter data is messy and filled with non-standard English words. There are slang, misspellings, abbreviations, hashtags, Twitter handles, links, emojis, and other things that make it difficult to analyze. A thorough cleaning is imperative.&lt;/p&gt;

&lt;p&gt;I made my own custom cleaning function to handle Twitter specific issues like correcting common misspellings, slang, eliminating emojis, Twitter handles etc. . .&lt;/p&gt;

&lt;p style=&quot;float: left; font-size: 9pt; text-align: center; width: 99%;
margin-right: 1%; margin-bottom: 0.5em;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-05-May/2019-05-18-What-Are-People-Saying-About-Airlines-on-Twitter/raw_tweet.png&quot; style=&quot;width: 100%&quot; /&gt;
Raw tweet, words with red squares will be cleaned&lt;/p&gt;

&lt;p style=&quot;float: left; font-size: 9pt; text-align: center; width: 99%;
margin-right: 1%; margin-bottom: 0.5em;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-05-May/2019-05-18-What-Are-People-Saying-About-Airlines-on-Twitter/cleaned_tweet.png&quot; style=&quot;width: 100%&quot; /&gt;
After the first iteration of cleaning the tweet&lt;/p&gt;

&lt;p&gt;This is the first of many iterations in the cleaning process. From here I would convert all words to lower case, remove “$” symbols, remove numbers, and other things to make tweets easier for a computer to work with.&lt;/p&gt;

&lt;p&gt;Next I took the cleaned tweets and converted them to a numerical format.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-18-What-Are-People-Saying-About-Airlines-on-Twitter/count_vectorizer.png&quot; alt=&quot;Count Vectorizer&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each row is a tweet and the words in the tweet would be tallied in the corresponding columns.&lt;/p&gt;

&lt;p&gt;After counting, I had a really, really long list of words and counts. I had a matrix that was&lt;/p&gt;

&lt;p&gt;61711 rows x 24583 columns&lt;/p&gt;

&lt;p&gt;The rows represent the number of tweets, the columns represent the number of unique words. How do we work with this data?&lt;/p&gt;

&lt;p&gt;We need to do dimensionality reduction. I like to think of dimensionality reduction as taking a lot of data that doesn’t mean much on its own and compressing it down such that the compressed data conveys much more information. Take movies for example. There are tens of thousands of movies, each one having its own unique features. We can go from a space where we think about every movie individually to one where we think in genres. Suddenly, we go from tens of thousands of movies to 30 genres.&lt;/p&gt;

&lt;p&gt;Similarly with words, we can take tens of thousands of words and compress them into “topics.”&lt;/p&gt;

&lt;p&gt;It turns out, it is much easier to analyze tweets in this reduced dimension “topic” space. In the new “topic” space, I can group our tweets into clusters.&lt;/p&gt;

&lt;p&gt;After clustering, I examine the tweets in each cluster and try to understand the “topic” of the cluster and if it makes sense.&lt;/p&gt;

&lt;p&gt;This process is repeated until the topics make sense. It is an iterative process. There is no right answer or score.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-18-What-Are-People-Saying-About-Airlines-on-Twitter/workflow.png&quot; alt=&quot;Workflow&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above is the flow chart for working the project. Start at the upper left. The process is iterative, which means it is repeated until the data scientist is satisfied with the results.&lt;/p&gt;

&lt;h2 id=&quot;summary-and-app&quot;&gt;Summary and App&lt;/h2&gt;
&lt;p&gt;One particularly interesting topic that I found was topic number 9 which was about racism. On October 25, American Airlines was flagged for racist policies. There were a huge spike in tweets directed towards them from angry tweeters. However, the controversy quickly died down and the angry tweets returned to normal within a few days.&lt;/p&gt;

&lt;p&gt;Another notable topic was topic number 3 which was about love of flying. It showed people’s enthusiasm for traveling and their positive experiences with airlines. Topic number 7 is a topic nearly every traveler is familiar with: waiting at the gate, worrying about time, and flight delays.&lt;/p&gt;

&lt;p&gt;Here is a link to an &lt;a href=&quot;https://airline-tweets.herokuapp.com/&quot; target=&quot;_blank&quot;&gt;Interactive App&lt;/a&gt; that summarizes my results.&lt;/p&gt;

&lt;p&gt;The code for the project can be found on my &lt;a href=&quot;https://github.com/ericchan24/airline_twitter&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Predicting Fake News With 99 Percent Accuracy</title>
   <link href="http://localhost:4000/classification/naive%20bayes/2019/05/17/Predicting-Fake-News-With-99-percent-Accuracy/"/>
   <updated>2019-05-17T00:00:00-05:00</updated>
   <id>http://localhost:4000/classification/naive%20bayes/2019/05/17/Predicting-Fake-News-With-99-percent-Accuracy</id>
   <content type="html">&lt;h2 id=&quot;tldr&quot;&gt;tl;dr&lt;/h2&gt;
&lt;p&gt;I built a classifier using AI and natural language processing that was 99% accurate in detecting whether news articles were real or fake news. I obtained the articles by web scraping real and fake news sites to download over 600 articles.&lt;/p&gt;

&lt;p style=&quot;float: left; font-size: 9pt; text-align: center; width: 49%;
margin-right: 1%; margin-bottom: 0.5em;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-05-May/2019-05-17-Predicting-Fake-News-With-99-Percent-Accuracy/real_news_wordcloud.png&quot; style=&quot;width: 100%&quot; /&gt;
Real News Words&lt;/p&gt;

&lt;p style=&quot;float: left; font-size: 9pt; text-align: center; width: 49%;
margin-right: 1%; margin-bottom: 0.5em;&quot;&gt;
&lt;img src=&quot;/assets/img/2019-05-May/2019-05-17-Predicting-Fake-News-With-99-Percent-Accuracy/fake_news_wordcloud.png&quot; style=&quot;width: 100%&quot; /&gt;
Fake News Words&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A large chemical company keeps its employees up to date on news by compiling a list of news articles to send to its staff. Recently, its news feed has been infiltrated by fake news articles. I used AI and Natural Language Processing to build a classifier to separate the genuine articles from the fake news articles.&lt;/p&gt;

&lt;h2 id=&quot;the-blacklist&quot;&gt;The Blacklist&lt;/h2&gt;
&lt;p&gt;The company provided a list of sites on its “blacklist”. These were sites that they flagged as sources of fake news. At first glance, articles on these sites appear to be real news. But closer examination reveals otherwise. Here is an excerpt of an article from one of the sites on the blacklist &lt;a href=&quot;http://nbpostgazette.com/&quot; target=&quot;_blank&quot;&gt;nbpostgazette&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote cite=&quot;http://nbpostgazette.com&quot;&gt;
Growing opportunity for startups in the Fuel Dispensers industry&lt;br /&gt;&lt;br /&gt;

Questale released a detailed assessment of trends in Fuel Dispensers market. The research report includes diverse topics like total market size, key market drivers, challenges, growth opportunities, key players etc. We have also covered key market updates, the impact of regulations and technological updates. New startups entering the space of Fuel Dispensers need to carefully pick their niches and genres so that they can compete on an equal footing with global companies who have an end to end development studios, production capabilities and global skills and experience backing them.

You can get free access to samples from the report here: https://questale.com/report/fuel-dispensers-market-report-by-company-regions-types-and-applications-global-status-and-forecast-to-2025/303479

&lt;/blockquote&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;building-a-data-set&quot;&gt;Building a data set&lt;/h2&gt;
&lt;p&gt;I built a web scraper to download over 600 news articles from real news sources (e.g. NY Times, FiveThirtyEight, Reuters) and fake news sources using the company’s blacklist (e.g. NBPostGazette, JournalismDay, SatPRNews).&lt;/p&gt;

&lt;p&gt;I prepared the articles for building a classifier by removing stop words (words such as &lt;strong&gt;and&lt;/strong&gt;, &lt;strong&gt;the&lt;/strong&gt;, and &lt;strong&gt;I&lt;/strong&gt; that don’t convey much information), converting all words to lowercase, and removing punctuation.&lt;/p&gt;

&lt;h2 id=&quot;counting-the-words-in-each-article&quot;&gt;Counting the words in each article&lt;/h2&gt;
&lt;p&gt;Counting the words in each article is the key step that quantifies our text data into a numerical format for modeling.&lt;/p&gt;

&lt;p&gt;I used a method of quantifying article word counts called term frequency inverse document frequency (TF-IDF).&lt;/p&gt;

&lt;p&gt;Term frequency represents how important each word is in an article. It is a rate statistic.&lt;/p&gt;

&lt;p&gt;Inverse document frequency penalizes words that appear in many articles. The idea is that words that occur in many articles are less informative than those
that occur only in a small number of the articles.&lt;/p&gt;

&lt;p&gt;TF-IDF multiplies term frequency by inverse document frequency to quantify the importance of each word in each article.&lt;/p&gt;

&lt;h2 id=&quot;naive-bayes&quot;&gt;Naive Bayes&lt;/h2&gt;
&lt;p&gt;Naive Bayes is a fast, accurate, and easy to implement classifier often used in text classification. It is based on Bayes’ theorem and the &lt;strong&gt;naive&lt;/strong&gt; part comes from the assumption that the features in the dataset are independent.&lt;/p&gt;

&lt;p&gt;We are interested in calculating the posterior probabilities: What is the
probability that an article is fake news given the words in the article?&lt;/p&gt;

&lt;p&gt;Naive Bayes looks at the TF-IDF score for each article and uses those numbers to identify words correlated with real news and fake news.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;The classifier misclassified one out of 127 articles in the test set. It correctly classified all 27 fake news articles in the test set.&lt;/p&gt;

&lt;p&gt;This is a &lt;a href=&quot;https://github.com/ericchan24/Fake-News-Classifier&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt; link with all the code used in this project.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Zion National Park Should be a UNESCO World Heritage Site</title>
   <link href="http://localhost:4000/classification/support%20vector%20machines/2019/05/16/Zion-National-Park-Should-Be-a-UNESCO-World-Heritage-Site/"/>
   <updated>2019-05-16T00:00:00-05:00</updated>
   <id>http://localhost:4000/classification/support%20vector%20machines/2019/05/16/Zion-National-Park-Should-Be-a-UNESCO-World-Heritage-Site</id>
   <content type="html">&lt;p&gt;&lt;img src=&quot;\assets\img\2019-05-May\2019-05-16-Zion-National-Park-Should-be-a-UNESCO-World-Heritage-Site\montage.png&quot; alt=&quot;Montage&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Zion National Park is a large and beautiful park in Utah. Millions of visitors visit annually to hike its scenic terrain. Why is Zion not on the list of UNESCO World Heritage Sites?&lt;/p&gt;

&lt;h2 id=&quot;what-is-unesco&quot;&gt;What is UNESCO?&lt;/h2&gt;

&lt;p&gt;UNESCO is an agency of the United Nations that promotes science, education, and culture. One of the ways they do this is with a list of World Heritage Sites. To make it on the list, a site must be judged of significance to the collective interests of humanity.&lt;/p&gt;

&lt;p&gt;I built a model using machine learning to classify whether a site would make it on the UNESCO list.&lt;/p&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;
&lt;p&gt;UNESCO sites are divided into two types, one is cultural. These are man-made sites such as The Statue of Liberty or The Great Wall of China. The other type is natural. Natural sites are parks, marine areas, and landforms that exhibit outstanding examples plant and animal life. I focused on the natural sites, specifically the parks. The parks have more consistent features that can be used to build a model.&lt;/p&gt;

&lt;h2 id=&quot;getting-the-data&quot;&gt;Getting the Data&lt;/h2&gt;
&lt;p&gt;I gathered data from UNESCO, the US National Park Service, the Canadian National Park Service, and TripAdvisor for reviews about each park. The US and Canadian Park Service gave me observations of parks not on the UNESCO list.&lt;/p&gt;

&lt;h2 id=&quot;features&quot;&gt;Features&lt;/h2&gt;
&lt;p&gt;The features that I looked at were size of the park, age of the park, TripAdvisor review score and Number of TripAdvisor reviews. The number of reviews was important because some sites had a very high rating.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-16-Zion-National-Park-Should-be-a-UNESCO-World-Heritage-Site/torngat_trip_advisor.PNG&quot; alt=&quot;High Rating&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But only one review.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-16-Zion-National-Park-Should-be-a-UNESCO-World-Heritage-Site/torngat_trip_advisor1.PNG&quot; alt=&quot;One Review&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So I combined review score and number of reviews into a single metric “weighted reviews” that accounts for the number of reviews. This also serves as proxy as number of visitors to a park that is important in the model.&lt;/p&gt;

&lt;h2 id=&quot;testing-out-machine-learning-algorithms&quot;&gt;Testing out Machine Learning Algorithms&lt;/h2&gt;
&lt;p&gt;I used F1 score as my scoring metric. A baseline model that always predicts a park is a UNESCO site had an F1 score of 49%. I trained six different machine learning models with my data and here are the results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-16-Zion-National-Park-Should-be-a-UNESCO-World-Heritage-Site/f1_score_by_model.PNG&quot; alt=&quot;F1 Scores by Model&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A support vector machine (SVM) model was the best performer with a F1 score of 62%.&lt;/p&gt;

&lt;h2 id=&quot;what-exactly-can-this-model-do&quot;&gt;What Exactly Can This Model Do?&lt;/h2&gt;
&lt;p&gt;Given a set of park features, I can input them into the model will look at the data I used to train it and output a prediction whether a park is a UNESCO World Heritage Site.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-16-Zion-National-Park-Should-be-a-UNESCO-World-Heritage-Site/prediction.png&quot; alt=&quot;Prediction&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The model predicts that Zion National Park should be classified as a UNESCO World Heritage Site.&lt;/p&gt;

&lt;h2 id=&quot;going-forward&quot;&gt;Going Forward&lt;/h2&gt;
&lt;p&gt;The model currently has a F1 score of 62% on 140 parks, 70 that were UNESCO sites and 70 that were non-UNESCO sites. I would like to add more parks from all around the world to see if I can increase the F1 score.&lt;/p&gt;

&lt;p&gt;With a strong enough score, the model would be used to find those parks that are not currently UNESCO sites that should be UNESCO sites.&lt;/p&gt;

&lt;p&gt;The code for the project can be found on my &lt;a href=&quot;https://github.com/ericchan24/unesco_classification&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Classifying Animals Using Machine Learning</title>
   <link href="http://localhost:4000/classification/random%20forest/2019/05/15/Classifying-Animals-Using-Machine-Learning/"/>
   <updated>2019-05-15T00:00:00-05:00</updated>
   <id>http://localhost:4000/classification/random%20forest/2019/05/15/Classifying-Animals-Using-Machine-Learning</id>
   <content type="html">&lt;head&gt;
&lt;style&gt;
.wrap {
    width: 300px;
    position: relative;
}

.wrap img {
    float: left;
    height: 20px;
}

.wrap h2 {
    line-height: 20px;
    &lt;!-- top: 33px;
    left: 50px; --&gt;
    &lt;!-- display: inline; --&gt;
}
tr.dark {
    background-color: #141866;
    color: #ffffff;
}
&lt;/style&gt;
&lt;/head&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;\assets\img\2019-05-May\2019-05-15-Classifying-Animals-Using-Machine-Learning\mini_highland_cow.png&quot; alt=&quot;Mini Highland Cow&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is a mini highland cow. From the image, we see that it has hair, a tail,
and four legs. Some other things that we know about this animal is that it has
teeth and it produces milk.&lt;/p&gt;

&lt;p&gt;The mini highland cow’s attributes are what are called the features of the
animal. The class it belongs to is the mammal class. My goal is to develop a
model that maps from the features of an animal to its class.&lt;/p&gt;

&lt;h2 id=&quot;the-data&quot;&gt;The Data&lt;/h2&gt;

&lt;p&gt;The Data
I used a data set from the &lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/zoo&quot; target=&quot;_blank&quot;&gt;UCI Machine Learning repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\assets\img\2019-05-May\2019-05-15-Classifying-Animals-Using-Machine-Learning\animal_classes.png&quot; alt=&quot;Animal Classes&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The animals are divided into seven classes. A particularly interesting class is
the non-insect invertebrate class. It includes are a variety of animals from
worms, arachnids, crustaceans, mollusks, and others.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\assets\img\2019-05-May\2019-05-15-Classifying-Animals-Using-Machine-Learning\number_of_animals_by_class.PNG&quot; alt=&quot;Number of Animals by Class&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are 101 animals in the data set. There were only five reptiles and four
amphibians. This is important to keep in mind during the splitting of the data.
I want at least one example of reptile and amphibian in my testing data.&lt;/p&gt;

&lt;h2 id=&quot;data-exploration-preprocessing-and-cleaning&quot;&gt;Data exploration, preprocessing, and cleaning&lt;/h2&gt;
&lt;p&gt;This is a clean data set with no missing values or outliers. The only non-binary feature was the number of legs of each animal. I scaled the number of legs to values between 0 and 1 using a max-min scaler.&lt;/p&gt;

&lt;h2 id=&quot;modeling&quot;&gt;Modeling&lt;/h2&gt;
&lt;p&gt;There are many algorithms to choose for classification. There is a rule of thumb for logistic regression requiring ten training observations per feature to have properly calibrated model probabilities. I decided to not use logistic regression. Naive Bayes has the assumption that the features used are independent. Since I had features such as “Does this animal have fins” and “Does this animal live in the water”, my features were not independent. I decided not to use naive bayes.&lt;/p&gt;

&lt;p&gt;I built four models and compared the results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\assets\img\2019-05-May\2019-05-15-Classifying-Animals-Using-Machine-Learning\four_model_accuracy_barh.PNG&quot; alt=&quot;Four Model Accuracy&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These models were built without any tuning of hyperparameters. Random forest
and decision tree were producing high accuracy and moved forward with these
algorithms.&lt;/p&gt;

&lt;p&gt;I used randomized grid search to tune the hyperparameters for a decision tree
and random forest model. Random forest produced the best results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\assets\img\2019-05-May\2019-05-15-Classifying-Animals-Using-Machine-Learning\confusion_matrix_random_forest.PNG&quot; alt=&quot;Confusion Matrix Random Forest&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The random forest model correctly classified every observation in the test set.&lt;/p&gt;

&lt;p&gt;Conclusion
I made a model that maps from animal features to the class of that animal using
random forest. The model was trained on 81 observations and tested on 20 observations. It had 100% accuracy on the test set.&lt;/p&gt;

&lt;p&gt;I made an &lt;a href=&quot;https://animal-classifier.herokuapp.com/&quot; target=&quot;_blank&quot;&gt;interactive app&lt;/a&gt; that shows how the model works.&lt;/p&gt;

&lt;p&gt;The code for this project can be found on my &lt;a href=&quot;https://github.com/ericchan24/Animal-Classification&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How Expensive is Your City?</title>
   <link href="http://localhost:4000/linear%20regression/2019/05/14/How-Expensive-is-Your-City/"/>
   <updated>2019-05-14T00:00:00-05:00</updated>
   <id>http://localhost:4000/linear%20regression/2019/05/14/How-Expensive-is-Your-City</id>
   <content type="html">&lt;head&gt;
&lt;style&gt;
.wrap {
    width: 300px;
    position: relative;
}

.wrap img {
    float: left;
    height: 20px;
}

.wrap h2 {
    line-height: 20px;
    &lt;!-- top: 33px;
    left: 50px; --&gt;
    &lt;!-- display: inline; --&gt;
}
tr.dark {
    background-color: #141866;
    color: #ffffff;
}
&lt;/style&gt;
&lt;/head&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Suppose you receive a job offer in a new city. Your new job will give you a raise by 10%. But will you be better off financially after moving to that city after taking into consideration its cost of living?&lt;/p&gt;

&lt;p&gt;The website &lt;a href=&quot;https://www.expatistan.com/cost-of-living&quot; target=&quot;_blank&quot;&gt;Expatistan&lt;/a&gt; has information about prices from over 320 cities around the world. Each city has a list of over 50 items and their corresponding prices. The website takes those items and prices and gives us a value for the price index of each city.&lt;/p&gt;

&lt;h2 id=&quot;objective&quot;&gt;Objective&lt;/h2&gt;
&lt;p&gt;I decided to take the idea of this website and extend it by building a model to predict the cost of living index of any city in the world.&lt;/p&gt;

&lt;p&gt;In order train my model, I needed to get data. I used Python’s BeautifulSoup library to build a web scraper, to scrape 320 individual city pages from Expatistan’s website. I extracted the item names and item prices from each page and loaded the data into a Pandas data frame.&lt;/p&gt;

&lt;p&gt;I cleaned up the data and built my model. I used scikit-learn to perform a linear regression to calculate the coefficients for my model. Here are the results of my model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/2019-05-May/2019-05-14-How-Expensive-is-Your-City/all_features.png&quot; alt=&quot;All Features Model Results&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I reverse engineered the website’s model. I was able to perfectly predict a city’s price index!&lt;/p&gt;

&lt;p&gt;However, my goal was to be able to use this model on all cities, not only the cities included on the website. The problem with this is that I need 50 prices of all sorts of items from in categories from food, housing, clothing, transportation, health, and entertainment. For some of these items would be nearly impossible to get their prices. There is no reliable to place to find the price of &lt;strong&gt;1 pair of men’s leather business shoes&lt;/strong&gt; or &lt;strong&gt;Short visit to private Doctor&lt;/strong&gt;. I went item-by-item and kept only the items for which I could easily find their prices.&lt;/p&gt;

&lt;p&gt;I was left with only six items: gasoline, rental housing, fast food, public transportation, internet, and automobiles. I had to build a new model with the combination of these items. I also created a new feature called “region” that was based on the location of the city because I thought location would affect a city’s price index.&lt;/p&gt;

&lt;p&gt;I was skeptical whether a new model using only seven items would be very predictive. I noticed that rent was highly correlated to price index and I built a test model using only this single feature. It turns out that rent by itself is highly predictive of price index! From here, I added the price of fast food, the price of a monthly bus pass, and the region in which the city was located as my final model.&lt;/p&gt;

&lt;p&gt;Here are the results of my final model:
&lt;img src=&quot;/assets/img/2019-05-May/2019-05-14-How-Expensive-is-Your-City/final_predictions.png&quot; alt=&quot;Final Model&quot; width=&quot;650px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;chicago-vs-south-bend&quot;&gt;Chicago vs South Bend&lt;/h2&gt;
&lt;p&gt;Here’s an example of the model in action. I’ll calculate the price indices for Chicago and compare it to South Bend.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# chicago
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;12.76&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Rent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.03&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Bus_ticket&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;105&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.42&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Fast_Food&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.57&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Region&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;14.43&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cost_chi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Intercept&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Rent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bus_ticket&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Fast_Food&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Region&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cost_chi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;184&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# south bend
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Intercept_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;12.76&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Rent_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;900&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.03&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Bus_ticket_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;36&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.42&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Fast_Food_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.57&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Region_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;14.43&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cost_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Intercept_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Rent_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bus_ticket_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Fast_Food_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Region_sb&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cost_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;109&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cost_chi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost_sb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.688&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;My model calculates that Chicago’s cost of living is about 69% more expensive than South Bend’s.&lt;/p&gt;

&lt;p&gt;The code for the project can be found on my &lt;a href=&quot;https://github.com/ericchan24/price_indices&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here’s a link to some &lt;a href=&quot;https://price-indices.herokuapp.com/&quot; target=&quot;_blank&quot;&gt;visualizations&lt;/a&gt; that I made for this project.&lt;/p&gt;
</content>
 </entry>
 
 
</feed>
